{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "36ad9a5a",
   "metadata": {},
   "source": [
    "# Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "246f4b2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords as nltk_stopwords\n",
    "import re\n",
    "import json\n",
    "import spacy\n",
    "\n",
    "with open('stopwords-iso.json', 'r', encoding='utf-8') as file:\n",
    "    stopwords_iso = json.load(file)\n",
    "\n",
    "stopwords = set(nltk_stopwords.words('english'))\n",
    "custom_stopwords = set(['server', 'joined','scroll','scrolls','papyrus','image'])\n",
    "stopwords.update(custom_stopwords)\n",
    "stopwords.update(stopwords_iso['en'])\n",
    "\n",
    "def preprocess_text(text):\n",
    "\n",
    "    # Convert text to lowercase\n",
    "    text = text.lower()\n",
    "\n",
    "    \n",
    "    # Load SpaCy model\n",
    "    nlp = spacy.load(\"en_core_web_sm\")\n",
    "    doc = nlp(text)\n",
    "\n",
    "    # Lemmatize and lowercase each token\n",
    "    text = ' '.join([token.lemma_.lower() for token in doc])\n",
    "    \n",
    "    # Remove function definitions\n",
    "    text = re.sub(r'\\b[a-zA-Z_][a-zA-Z0-9_]*\\s*\\([^)]*\\)\\s*', '', text)\n",
    "    \n",
    "    # Remove URLs\n",
    "    text = re.sub(r'http[s]?://\\S+', '', text)\n",
    "\n",
    "    # Remove file names with common extensions\n",
    "    text = re.sub(r'\\b\\w+\\.(zip|tif|pdf|jpg|png|docx|xlsx|rar|txt|csv|json)\\b', '', text)\n",
    "\n",
    "    # Remove numbers\n",
    "    text = re.sub(r'\\d+', '', text)\n",
    "\n",
    "    # Remove punctuation\n",
    "    text = re.sub(r'[^\\w\\s]', '', text)\n",
    "\n",
    "    # Remove special characters and emojis\n",
    "    text = re.sub(r'[\\U0001F600-\\U0001F64F'\n",
    "              r'\\U0001F300-\\U0001F5FF'  \n",
    "              r'\\U0001F680-\\U0001F6FF'  \n",
    "              r'\\U0001F700-\\U0001F77F'  \n",
    "              r'\\U0001F780-\\U0001F7FF'  \n",
    "              r'\\U0001F800-\\U0001F8FF'  \n",
    "              r'\\U0001F900-\\U0001F9FF'  \n",
    "              r'\\U0001FA00-\\U0001FA6F'  \n",
    "              r'\\U0001FA70-\\U0001FAFF'  \n",
    "              r'\\u2600-\\u26FF'          \n",
    "              r'\\u2700-\\u27BF'       \n",
    "              ']+', '', text)\n",
    "\n",
    "    # Remove stopwords\n",
    "    tokens = text.split()\n",
    "    tokens = [token for token in tokens if token not in stopwords]\n",
    "    \n",
    "    return ' '.join(tokens)\n",
    "\n",
    "def preprocess_json(input_file, output_file):\n",
    "    with open(input_file, 'r', encoding='utf-8') as file:\n",
    "        data = json.load(file)\n",
    "\n",
    "    for message in data.get('messages', []):\n",
    "        if message.get('content'):\n",
    "            message['content'] = preprocess_text(message['content'])\n",
    "\n",
    "    with open(output_file, 'w', encoding='utf-8') as file:\n",
    "        json.dump(data, file, indent=4)\n",
    "\n",
    "    print(f'Preprocessed data has been saved to {output_file}')\n",
    "\n",
    " \n",
    "def preprocess_all_files(input_folder, output_folder):\n",
    "    for filename in os.listdir(input_folder):\n",
    "        input_file = os.path.join(input_folder, filename)\n",
    "        output_file = os.path.join(output_folder, filename)\n",
    "        preprocess_json(input_file, output_file)\n",
    "\n",
    "\n",
    "input_folder = 'filteredJSON'\n",
    "output_folder = 'preprocessedJSON'\n",
    "\n",
    "preprocess_all_files(input_folder, output_folder)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c320feb",
   "metadata": {},
   "source": [
    "# Organizing Messages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3812f261",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "input_folder = 'preprocessedJSON'\n",
    "def get_all_documents(folder):\n",
    "    all_docs = []\n",
    "    files =  {'Vesuvius Challenge - Text Channels - papyrology [1108134343295127592]_filtered.json', \n",
    "              'Vesuvius Challenge - Text Channels - general [1079907750265499772]_filtered.json'} #for looking at specific files, can customize\n",
    "\n",
    "    for fname in os.listdir(folder):\n",
    "        if fname not in files:\n",
    "            continue\n",
    "\n",
    "        with open(os.path.join(folder, fname), 'r', encoding='utf-8') as f:\n",
    "            data = json.load(f)\n",
    "            channel = data.get(\"channel\", \"Unknown\")\n",
    "            channel_name = channel.get(\"name\") if isinstance(channel, dict) else channel\n",
    "\n",
    "            for msg in data.get(\"messages\", []):\n",
    "                content = msg.get(\"content\", \"\")\n",
    "                if content.strip():  # Only include non-empty messages\n",
    "                    all_docs.append({\n",
    "                        \"channel\": channel_name,\n",
    "                        \"user\": msg.get(\"author\", {}).get(\"name\", \"Unknown\"),\n",
    "                        \"timestamp\": msg.get(\"timestamp\", \"Unknown\"),\n",
    "                        \"content\": content\n",
    "                    })\n",
    "\n",
    "    return all_docs\n",
    "\n",
    "\n",
    "df = pd.DataFrame(get_all_documents(input_folder))\n",
    "\n",
    "df['timestamp'] = pd.to_datetime(df['timestamp'], errors='coerce')\n",
    "# Remove timezone only if present\n",
    "if df['timestamp'].dt.tz is not None:\n",
    "    df['timestamp'] = df['timestamp'].dt.tz_localize(None)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddf58d85",
   "metadata": {},
   "source": [
    "# Term Frequency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4314e678",
   "metadata": {},
   "outputs": [],
   "source": [
    "# For general, papyrology, and both together\n",
    "import pandas as pd\n",
    "from collections import Counter\n",
    "from nltk.util import ngrams\n",
    "import spacy\n",
    "spacy.cli.download(\"en_core_web_sm\")\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "def tokenize(text):\n",
    "    doc = nlp(text.lower())\n",
    "    return [token.text for token in doc if token.is_alpha]\n",
    "\n",
    "# Frequency counter\n",
    "def get_top_ngrams(texts, n=1, top_k=20):\n",
    "    all_tokens = []\n",
    "    for text in texts:\n",
    "        all_tokens.extend(tokenize(text))\n",
    "    if n == 1:\n",
    "        return Counter(all_tokens).most_common(top_k)\n",
    "    else:\n",
    "        return Counter(ngrams(all_tokens, n)).most_common(top_k)\n",
    "\n",
    "# Split by channel\n",
    "general_msgs = df[df['channel'].str.lower() == 'general']['content'].dropna().tolist()\n",
    "papyrology_msgs = df[df['channel'].str.lower() == 'papyrology']['content'].dropna().tolist()\n",
    "combined_msgs = general_msgs + papyrology_msgs\n",
    "\n",
    "# Get n-grams\n",
    "unigrams_combined = get_top_ngrams(combined_msgs, n=1)\n",
    "bigrams_combined = get_top_ngrams(combined_msgs, n=2)\n",
    "\n",
    "unigrams_general = get_top_ngrams(general_msgs, n=1)\n",
    "unigrams_papyrology = get_top_ngrams(papyrology_msgs, n=1)\n",
    "\n",
    "bigrams_general = get_top_ngrams(general_msgs, n=2)\n",
    "bigrams_papyrology = get_top_ngrams(papyrology_msgs, n=2)\n",
    "\n",
    "# Convert to DataFrames\n",
    "def ngram_df(ngrams_list, label='ngram'):\n",
    "    return pd.DataFrame(ngrams_list, columns=[label, 'count'])\n",
    "\n",
    "# Write to Excel\n",
    "with pd.ExcelWriter('channel_analysis.xlsx') as writer:\n",
    "    df.to_excel(writer, sheet_name='Messages', index=False)\n",
    "    ngram_df(bigrams_combined, 'bigram').to_excel(writer, sheet_name='Bigrams Combined', index=False)\n",
    "    \n",
    "    # Separate bigrams\n",
    "    bigrams_sep = pd.concat([\n",
    "        ngram_df(bigrams_general, 'bigram').assign(channel='general'),\n",
    "        ngram_df(bigrams_papyrology, 'bigram').assign(channel='papyrology')\n",
    "    ])\n",
    "    bigrams_sep.to_excel(writer, sheet_name='Bigrams Separate', index=False)\n",
    "    \n",
    "    # Combined unigrams\n",
    "    ngram_df(unigrams_combined, 'unigram').to_excel(writer, sheet_name='Unigrams Combined', index=False)\n",
    "    \n",
    "    # Separate unigrams\n",
    "    unigrams_sep = pd.concat([\n",
    "        ngram_df(unigrams_general, 'unigram').assign(channel='general'),\n",
    "        ngram_df(unigrams_papyrology, 'unigram').assign(channel='papyrology')\n",
    "    ])\n",
    "    unigrams_sep.to_excel(writer, sheet_name='Unigrams Separate', index=False)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
