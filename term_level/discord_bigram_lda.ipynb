{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c19dbad9",
   "metadata": {},
   "source": [
    "# Discord Bigram LDA â€” **Notebook Version**\n",
    "\n",
    "This notebook reproduces your bigram LDA pipeline while **matching preprocessing** with the settings found in your `term_level_streamlined.ipynb`:\n",
    "- `lowercase=True`, `keep_only_alpha=True`, `min_token_len=2`, `use_lemma=True`, `remove_numbers=True`\n",
    "- NLTK stopwords **+** your `extra_stopwords` **+** `stopwords-iso.json` (if present)\n",
    "- System message filters (same patterns)\n",
    "\n",
    "> Generated automatically."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b513e509",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-11T15:13:30.073789Z",
     "start_time": "2025-11-11T15:13:21.958139Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/nikhil/PycharmProjects/vesuvius_discord_study/.venv/lib/python3.9/site-packages/urllib3/__init__.py:35: NotOpenSSLWarning: urllib3 v2 only supports OpenSSL 1.1.1+, currently the 'ssl' module is compiled with 'LibreSSL 2.8.3'. See: https://github.com/urllib3/urllib3/issues/3020\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# --- Config (ported to match your existing notebook) ---\n",
    "import os\n",
    "import re\n",
    "import json\n",
    "import math\n",
    "from collections import Counter\n",
    "from typing import Iterable, List, Tuple, Callable, Optional, Set, Dict, Any, Union\n",
    "\n",
    "# Optional deps: NLTK stopwords\n",
    "try:\n",
    "    from nltk.corpus import stopwords as nltk_stop\n",
    "    _NLTK_AVAILABLE = True\n",
    "except Exception:\n",
    "    _NLTK_AVAILABLE = False\n",
    "\n",
    "# Optional deps: spaCy lemmatization\n",
    "try:\n",
    "    import spacy\n",
    "    _SPACY_AVAILABLE = True\n",
    "    try:\n",
    "        nlp = spacy.load(\"en_core_web_sm\")\n",
    "    except Exception:\n",
    "        # model not available; fall back to simple token logic\n",
    "        _SPACY_AVAILABLE = False\n",
    "        nlp = None\n",
    "except Exception:\n",
    "    _SPACY_AVAILABLE = False\n",
    "    nlp = None\n",
    "\n",
    "\n",
    "class Cfg: \n",
    "    # Flags\n",
    "    lowercase: bool = True\n",
    "    keep_only_alpha: bool = True\n",
    "    min_token_len: int = 2\n",
    "    use_lemma: bool = True\n",
    "    remove_numbers: bool = True\n",
    "\n",
    "    # Stopwords\n",
    "    use_nltk_stopwords: bool = True\n",
    "    extra_stopwords: Set[str] = {\n",
    "        \"server\", \"joined\", \"scroll\", \"papyrus\", \"image\", \"brett\", \"olsen\",\n",
    "        \"entry\", \"start\", \"thread\", \"moshe\", \"levy\", \"casey\", \"handmer\", \"mae\",\n",
    "        \"sawatzky\", \"like\", \"value\", \"seldon\", \"ben\"\n",
    "    }\n",
    "\n",
    "    # stopwords-iso (optional)\n",
    "    stopwords_iso: Dict[str, List[str]] = {}\n",
    "    try:\n",
    "        with open(\"stopwords-iso.json\", \"r\", encoding=\"utf-8\") as _f:\n",
    "            stopwords_iso = json.load(_f)\n",
    "    except Exception:\n",
    "        stopwords_iso = {}\n",
    "\n",
    "    # Filters / regexes\n",
    "    system_message_patterns = [\n",
    "        re.compile(r\"^\\s*Joined the server\\.?\\s*$\", re.I),\n",
    "        re.compile(r\"^\\s*Started a thread\\.?\\s*$\", re.I),\n",
    "    ]\n",
    "    url_inline_re = re.compile(r\"(https?://\\S+|www\\.\\S+)\", re.I)\n",
    "    url_only_re = re.compile(r\"^\\s*(https?://\\S+\\s*)+$\", re.I)\n",
    "    emoji_re = re.compile(\n",
    "        \"[\"                      # emojis\n",
    "        \"\\U0001F300-\\U0001F5FF\"\n",
    "        \"\\U0001F600-\\U0001F64F\"\n",
    "        \"\\U0001F680-\\U0001F6FF\"\n",
    "        \"\\U0001F700-\\U0001F77F\"\n",
    "        \"\\U0001F780-\\U0001F7FF\"\n",
    "        \"\\U0001F800-\\U0001F8FF\"\n",
    "        \"\\U0001F900-\\U0001F9FF\"\n",
    "        \"\\U0001FA00-\\U0001FA6F\"\n",
    "        \"\\U0001FA70-\\U0001FAFF\"\n",
    "        \"\\U00002702-\\U000027B0\"\n",
    "        \"\\U000024C2-\\U0001F251\"\n",
    "        \"]+\"\n",
    "    )\n",
    "    greek_re = re.compile(r\"[\\u0370-\\u03FF\\u1F00-\\u1FFF]\")\n",
    "\n",
    "\n",
    "cfg = Cfg()\n",
    "\n",
    "\n",
    "def contains_greek(text: str) -> bool:\n",
    "    return bool(cfg.greek_re.search(text or \"\"))\n",
    "\n",
    "\n",
    "# --- Stopwords assembly (NLTK + extra + stopwords-iso) ---\n",
    "def build_stopword_set() -> Set[str]:\n",
    "    sw: Set[str] = set()\n",
    "\n",
    "    # Include NLTK stopwords\n",
    "    if cfg.use_nltk_stopwords and _NLTK_AVAILABLE:\n",
    "        try:\n",
    "            sw |= set(nltk_stop.words(\"english\"))\n",
    "        except Exception:\n",
    "            pass\n",
    "\n",
    "    # Include extra custom stopwords\n",
    "    sw |= set(cfg.extra_stopwords)\n",
    "\n",
    "    # Include stopwords from stopwords-iso.json (English + fallback for others)\n",
    "    if isinstance(cfg.stopwords_iso, dict):\n",
    "        if \"en\" in cfg.stopwords_iso:\n",
    "            sw |= set(cfg.stopwords_iso[\"en\"])\n",
    "        else:\n",
    "            # fallback: flatten all lists if language keys differ\n",
    "            for lang_words in cfg.stopwords_iso.values():\n",
    "                try:\n",
    "                    sw |= set(lang_words)\n",
    "                except Exception:\n",
    "                    pass\n",
    "\n",
    "    # Normalize everything to lowercase\n",
    "    return {w.lower() for w in sw}\n",
    "\n",
    "\n",
    "STOPWORDS = build_stopword_set()\n",
    "# print(len(STOPWORDS))  # uncomment for sanity count\n",
    "\n",
    "\n",
    "# --- System / URL / emoji cleaning ---\n",
    "def is_system_message(text: str) -> bool:\n",
    "    t = text or \"\"\n",
    "    for pat in cfg.system_message_patterns:\n",
    "        if pat.search(t):\n",
    "            return True\n",
    "    return False\n",
    "\n",
    "\n",
    "def strip_urls(text: str) -> str:\n",
    "    t = text or \"\"\n",
    "    if cfg.url_only_re.match(t):\n",
    "        return \"\"\n",
    "    return cfg.url_inline_re.sub(\" \", t)\n",
    "\n",
    "\n",
    "def strip_emoji(text: str) -> str:\n",
    "    return cfg.emoji_re.sub(\" \", text or \"\")\n",
    "\n",
    "\n",
    "def clean_text(text: str) -> str:\n",
    "    if not text:\n",
    "        return \"\"\n",
    "    if is_system_message(text):\n",
    "        return \"\"\n",
    "    t = strip_urls(text)\n",
    "    t = strip_emoji(t)\n",
    "    t = t.replace(\"â€™\", \"'\")  # normalize apostrophe\n",
    "    return re.sub(r\"\\s+\", \" \", t).strip()\n",
    "\n",
    "\n",
    "# --- Tokenization / normalization (spaCy if available) ---\n",
    "def doc_to_tokens(doc_text: str, nlp_obj: Any, stopwords: Set[str]) -> List[str]:\n",
    "    t = clean_text(doc_text)\n",
    "    if not t:\n",
    "        return []\n",
    "    t = t.lower()  # lowercase before spaCy to stabilize POS/lemmatization\n",
    "\n",
    "    toks: List[str] = []\n",
    "    if _SPACY_AVAILABLE and nlp_obj is not None:\n",
    "        sp = nlp_obj(t)\n",
    "        for token in sp:\n",
    "            txt = token.text\n",
    "            if cfg.keep_only_alpha and not txt.isalpha():\n",
    "                continue\n",
    "            if cfg.remove_numbers and any(ch.isdigit() for ch in txt):\n",
    "                continue\n",
    "\n",
    "            raw = txt.lower() if cfg.lowercase else txt\n",
    "            norm = token.lemma_.lower() if (cfg.use_lemma and token.lemma_) else raw\n",
    "\n",
    "            if contains_greek(norm):\n",
    "                norm = \"GREEK_LETTERS\"  # keep uppercase on purpose\n",
    "\n",
    "            if len(norm) < cfg.min_token_len:\n",
    "                continue\n",
    "            if norm in stopwords:\n",
    "                continue\n",
    "            toks.append(norm)\n",
    "    else:\n",
    "        # Simple fallback (no lemma)\n",
    "        words = re.findall(r\"(?u)\\b[\\w']+\\b\", t)\n",
    "        for w in words:\n",
    "            if cfg.keep_only_alpha and not w.isalpha():\n",
    "                continue\n",
    "            if cfg.remove_numbers and any(ch.isdigit() for ch in w):\n",
    "                continue\n",
    "            tok = w.lower() if cfg.lowercase else w\n",
    "            if contains_greek(tok):\n",
    "                tok = \"GREEK_LETTERS\"\n",
    "            if len(tok) < cfg.min_token_len:\n",
    "                continue\n",
    "            if tok in stopwords:\n",
    "                continue\n",
    "            toks.append(tok)\n",
    "\n",
    "    return toks\n",
    "\n",
    "\n",
    "def docs_to_token_lists(texts: Iterable[str]) -> List[List[str]]:\n",
    "    return [doc_to_tokens(t, nlp, STOPWORDS) for t in texts]\n",
    "\n",
    "\n",
    "def bigrams_from_tokens_per_doc(tokens_per_doc: List[List[str]]) -> List[List[Tuple[str, str]]]:\n",
    "    out: List[List[Tuple[str, str]]] = []\n",
    "    for toks in tokens_per_doc:\n",
    "        out.append([(toks[i], toks[i + 1]) for i in range(len(toks) - 1)] if len(toks) > 1 else [])\n",
    "    return out\n",
    "\n",
    "\n",
    "def unigram_counts(tokens_per_doc: List[List[str]]) -> Tuple[Counter, Counter]:\n",
    "    tf, df = Counter(), Counter()\n",
    "    for toks in tokens_per_doc:\n",
    "        tf.update(toks)\n",
    "        df.update(set(toks))\n",
    "    return tf, df\n",
    "\n",
    "\n",
    "def bigram_counts(bigrams_per_doc: List[List[Tuple[str, str]]]) -> Tuple[Counter, Counter]:\n",
    "    tf, df = Counter(), Counter()\n",
    "    for bigs in bigrams_per_doc:\n",
    "        tf.update(bigs)\n",
    "        df.update(set(bigs))\n",
    "    return tf, df\n",
    "\n",
    "\n",
    "def mutual_information_bigrams(unigram_tf: Counter, bigram_tf: Counter, smoothing: int = 1) -> Dict[Tuple[str, str], float]:\n",
    "    T = sum(bigram_tf.values()) + smoothing\n",
    "    mi: Dict[Tuple[str, str], float] = {}\n",
    "    for (w1, w2), c12 in bigram_tf.items():\n",
    "        c1 = unigram_tf.get(w1, 0)\n",
    "        c2 = unigram_tf.get(w2, 0)\n",
    "        num = (c12 + smoothing) * T\n",
    "        den = (c1 + smoothing) * (c2 + smoothing)\n",
    "        mi[(w1, w2)] = math.log2(float(num) / float(den))\n",
    "    return mi\n",
    "\n",
    "\n",
    "# --- Corpus analysis (unigrams, bigrams, MI) ---\n",
    "def analyze_corpus(\n",
    "    docs: Union[Iterable[str], List[List[str]]],\n",
    "    top_k: int = 5000,\n",
    "    output_dir: Optional[str] = None,\n",
    "    pretokenized: bool = False\n",
    ") -> Dict[str, Any]:\n",
    "\n",
    "    import pandas as pd\n",
    "\n",
    "    if pretokenized:\n",
    "        tokens_per_doc = docs  # type: ignore[assignment]\n",
    "    else:\n",
    "        tokens_per_doc = docs_to_token_lists(docs)  # type: ignore[arg-type]\n",
    "\n",
    "    bigrams_per_doc = bigrams_from_tokens_per_doc(tokens_per_doc)\n",
    "    uni_tf, uni_df = unigram_counts(tokens_per_doc)\n",
    "    bi_tf, bi_df = bigram_counts(bigrams_per_doc)\n",
    "\n",
    "    df_uni = (\n",
    "        pd.DataFrame([(t, uni_tf[t], uni_df[t]) for t in uni_tf], columns=[\"term\", \"tf\", \"df\"])\n",
    "        .sort_values([\"tf\", \"df\", \"term\"], ascending=[False, False, True])\n",
    "        .head(top_k)\n",
    "    )\n",
    "    df_bi = (\n",
    "        pd.DataFrame([(\" \".join(p), bi_tf[p], bi_df[p]) for p in bi_tf], columns=[\"bigram\", \"tf\", \"df\"])\n",
    "        .sort_values([\"tf\", \"df\", \"bigram\"], ascending=[False, False, True])\n",
    "        .head(top_k)\n",
    "    )\n",
    "\n",
    "    mi = mutual_information_bigrams(uni_tf, bi_tf, smoothing=1)\n",
    "    df_mi = (\n",
    "        pd.DataFrame([(f\"{w1} {w2}\", bi_tf[(w1, w2)], m) for (w1, w2), m in mi.items()],\n",
    "                     columns=[\"bigram\", \"tf\", \"mi\"])\n",
    "        .sort_values([\"mi\", \"tf\", \"bigram\"], ascending=[False, False, True])\n",
    "        .head(top_k)\n",
    "    )\n",
    "\n",
    "    if output_dir:\n",
    "        os.makedirs(output_dir, exist_ok=True)\n",
    "        df_uni.to_csv(os.path.join(output_dir, \"unigrams.csv\"), index=False)\n",
    "        df_bi.to_csv(os.path.join(output_dir, \"bigrams.csv\"), index=False)\n",
    "        df_mi.to_csv(os.path.join(output_dir, \"bigrams_mi.csv\"), index=False)\n",
    "\n",
    "    return {\"unigrams\": df_uni, \"bigrams\": df_bi, \"bigrams_mi\": df_mi}\n",
    "\n",
    "\n",
    "# --- JSON loading with tokens and cleaned context ---\n",
    "def load_json_records_with_tokens(folder: str) -> List[Dict[str, Any]]:\n",
    "\n",
    "    records: List[Dict[str, Any]] = []\n",
    "    try:\n",
    "        files = [f for f in os.listdir(folder) if f.lower().endswith(\".json\")]\n",
    "    except Exception as e:\n",
    "        print(f\"âš ï¸ Could not list folder '{folder}': {e}\")\n",
    "        return records\n",
    "\n",
    "    for fname in files:\n",
    "        fpath = os.path.join(folder, fname)\n",
    "        try:\n",
    "            with open(fpath, \"r\", encoding=\"utf-8\") as f:\n",
    "                data = json.load(f)\n",
    "        except Exception as e:\n",
    "            print(f\"âš ï¸ Skipped {fname}: {e}\")\n",
    "            continue\n",
    "\n",
    "        channel = (data.get(\"channel\", {}) or {}).get(\"name\") or data.get(\"name\") or os.path.splitext(fname)[0]\n",
    "        for m in (data.get(\"messages\", []) or []):\n",
    "            raw = (m.get(\"content\") or \"\").strip()\n",
    "            if not raw:\n",
    "                continue\n",
    "\n",
    "            tokens = doc_to_tokens(raw, nlp, STOPWORDS)\n",
    "            if not tokens:\n",
    "                continue\n",
    "\n",
    "            author = (m.get(\"author\") or {}).get(\"name\") or (m.get(\"author\") or {}).get(\"id\") or \"Unknown\"\n",
    "            ts = m.get(\"timestamp\") or \"\"\n",
    "            records.append({\n",
    "                \"author\": author,\n",
    "                \"timestamp\": ts,\n",
    "                \"channel\": channel,\n",
    "                \"raw\": raw,\n",
    "                \"tokens\": tokens,\n",
    "                \"context\": \", \".join(tokens),  # comma-separated tokens\n",
    "            })\n",
    "\n",
    "    print(f\"Loaded {len(records)} normalized messages from {len(files)} files.\")\n",
    "    return records\n",
    "\n",
    "# --- Example usage (commented) ---\n",
    "# records = load_json_records_with_tokens(\"/path/to/filtered_JSON\")\n",
    "# docs_tokens = [r[\"tokens\"] for r in records]\n",
    "# results = analyze_corpus(docs_tokens, top_k=100000, output_dir=\"/path/to/outputs\", pretokenized=True)\n",
    "# import pandas as pd\n",
    "# df_msgs = pd.DataFrame([{k: r[k] for k in [\"author\",\"timestamp\",\"channel\",\"raw\",\"context\"]} for r in records],\n",
    "#                        columns=[\"author\",\"timestamp\",\"channel\",\"raw\",\"context\"])\n",
    "# df_msgs.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1463e5f7",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-11T15:13:30.083652Z",
     "start_time": "2025-11-11T15:13:30.081364Z"
    }
   },
   "outputs": [],
   "source": [
    "# --- System message filters ---\n",
    "system_message_patterns = [\n",
    "        re.compile(r\"^\\s*Joined the server\\.?\\s*$\", re.I),\n",
    "        re.compile(r\"^\\s*Started a thread\\.?\\s*$\", re.I),\n",
    "    ]\n",
    "\n",
    "def is_system_message(text: str) -> bool:\n",
    "    for pat in system_message_patterns:\n",
    "        if pat.match(text or \"\"):\n",
    "            return True\n",
    "    return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2e8a20e5",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-11T15:13:30.158015Z",
     "start_time": "2025-11-11T15:13:30.087862Z"
    }
   },
   "outputs": [],
   "source": [
    "# --- Load Discord JSON and clean ---\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "URL_ONLY_RE = re.compile(r\"^\\s*(?:https?://\\S+|www\\.\\S+)(?:\\s+(?:https?://\\S+|www\\.\\S+))*\\s*$\", re.I)\n",
    "URL_INLINE_RE = re.compile(r\"(https?://\\S+|www\\.\\S+)\", re.I)\n",
    "\n",
    "def load_discord_dir(input_dir: str) -> pd.DataFrame:\n",
    "    rows = []\n",
    "    p = os.path.expanduser(input_dir)\n",
    "    for root, _, files in os.walk(p):\n",
    "        for fname in files:\n",
    "            if not fname.lower().endswith(\".json\"):\n",
    "                continue\n",
    "            fpath = os.path.join(root, fname)\n",
    "            try:\n",
    "                with open(fpath, \"r\", encoding=\"utf-8\") as f:\n",
    "                    data = json.load(f)\n",
    "            except Exception:\n",
    "                continue\n",
    "            guild = data.get(\"guild\", {}) or {}\n",
    "            channel = data.get(\"channel\", {}) or {}\n",
    "            for m in (data.get(\"messages\", []) or []):\n",
    "                txt = m.get(\"content\")\n",
    "                rows.append({\n",
    "                    \"guild_id\": guild.get(\"id\"),\n",
    "                    \"guild_name\": guild.get(\"name\"),\n",
    "                    \"channel_id\": channel.get(\"id\"),\n",
    "                    \"channel_name\": channel.get(\"name\"),\n",
    "                    \"category\": channel.get(\"category\"),\n",
    "                    \"message_id\": m.get(\"id\"),\n",
    "                    \"timestamp\": m.get(\"timestamp\"),\n",
    "                    \"text\": \"\" if txt is None else str(txt),\n",
    "                })\n",
    "    df = pd.DataFrame(rows)\n",
    "    if df.empty:\n",
    "        return df\n",
    "    df[\"timestamp\"] = pd.to_datetime(df[\"timestamp\"], errors=\"coerce\", utc=True)\n",
    "    df[\"text\"] = df[\"text\"].astype(str)\n",
    "    return df\n",
    "\n",
    "def clean_messages(df: pd.DataFrame, min_doc_len: int = 10) -> pd.DataFrame:\n",
    "    if df.empty:\n",
    "        return df\n",
    "    df = df.copy()\n",
    "    # base trimming\n",
    "    df[\"text\"] = df[\"text\"].fillna(\"\").astype(str).str.strip()\n",
    "    df = df[df[\"text\"] != \"\"]\n",
    "    # system boilerplate\n",
    "    df = df[~df[\"text\"].apply(is_system_message)]\n",
    "    # url-only rows\n",
    "    df = df[~df[\"text\"].str.fullmatch(URL_ONLY_RE, na=False)]\n",
    "    # strip inline URLs\n",
    "    df[\"text\"] = df[\"text\"].str.replace(URL_INLINE_RE, \"\", regex=True).str.strip()\n",
    "    # normalize curly apostrophes\n",
    "    df[\"text\"] = df[\"text\"].str.replace(\"â€™\", \"'\", regex=False)\n",
    "    # enforce min length\n",
    "    df = df[df[\"text\"].str.len() >= int(min_doc_len)]\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "38245b36",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-11T15:13:30.167099Z",
     "start_time": "2025-11-11T15:13:30.163518Z"
    }
   },
   "outputs": [],
   "source": [
    "# --- Tokenization & analyzer (matches flags from your notebook) ---\n",
    "WORD_RE = re.compile(r\"(?u)\\b[\\w']+\\b\")\n",
    "\n",
    "def _lemma_tokens_spacy(text: str):\n",
    "    if not _SPACY_AVAILABLE or nlp is None:\n",
    "        return None\n",
    "    doc = nlp(text)\n",
    "    out = []\n",
    "    for t in doc:\n",
    "        w = t.lemma_ if getattr(cfg, \"use_lemma\", True) else t.text\n",
    "        out.append(w)\n",
    "    return out\n",
    "\n",
    "def base_tokens(text: str):\n",
    "    t = text or \"\"\n",
    "    if getattr(cfg, \"lowercase\", True):\n",
    "        t = t.lower()\n",
    "    # optional: remove digits before tokenizing\n",
    "    if getattr(cfg, \"remove_numbers\", True):\n",
    "        t = re.sub(r\"\\d+\", \" \", t)\n",
    "    # spaCy lemma fallback\n",
    "    toks = _lemma_tokens_spacy(t)\n",
    "    if toks is None:\n",
    "        toks = WORD_RE.findall(t)\n",
    "    # keep-only-alpha filter\n",
    "    if getattr(cfg, \"keep_only_alpha\", True):\n",
    "        toks = [w for w in toks if re.fullmatch(r\"[a-zA-Z']+\", w or \"\")]\n",
    "    # min token length\n",
    "    mlen = int(getattr(cfg, \"min_token_len\", 2))\n",
    "    toks = [w for w in toks if isinstance(w, str) and len(w) >= mlen]\n",
    "    # stopwords\n",
    "    toks = [w for w in toks if w not in STOPWORDS]\n",
    "    return toks\n",
    "\n",
    "def bigram_analyzer(doc: str):\n",
    "    toks = base_tokens(doc)\n",
    "    return [f\"{a} {b}\" for a, b in zip(toks, toks[1:])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c9054bef",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-11T15:13:30.181807Z",
     "start_time": "2025-11-11T15:13:30.178177Z"
    }
   },
   "outputs": [],
   "source": [
    "# --- Time aggregation (per-channel buckets) ---\n",
    "def aggregate_messages(df: pd.DataFrame, window: str) -> pd.DataFrame:\n",
    "    dfa = df.copy()\n",
    "    dfa[\"bucket_start\"] = pd.to_datetime(dfa[\"timestamp\"], errors=\"coerce\", utc=True).dt.floor(window)\n",
    "    keys = [\"channel_id\", \"bucket_start\"]\n",
    "    agg = (\n",
    "        dfa.sort_values(\"timestamp\")\n",
    "           .groupby(keys, as_index=False)\n",
    "           .agg(\n",
    "               text=(\"text\", lambda s: \" \".join([t for t in s if isinstance(t, str) and t.strip()])),\n",
    "               channel_name=(\"channel_name\", \"first\"),\n",
    "               category=(\"category\", \"first\"),\n",
    "               n_msgs=(\"text\", \"count\")\n",
    "           )\n",
    "    )\n",
    "    agg = agg[agg[\"text\"].str.len() >= 10]\n",
    "    return agg\n",
    "\n",
    "def label_corpora(df_docs: pd.DataFrame, pap_pat: str, gen_pat: str) -> pd.Series:\n",
    "    papy = df_docs[\"channel_name\"].fillna(\"\").str.contains(pap_pat, case=False, regex=True) | \\\n",
    "           df_docs[\"category\"].fillna(\"\").str.contains(pap_pat, case=False, regex=True)\n",
    "    gen  = df_docs[\"channel_name\"].fillna(\"\").str.contains(gen_pat, case=False, regex=True) | \\\n",
    "           df_docs[\"category\"].fillna(\"\").str.contains(gen_pat, case=False, regex=True)\n",
    "    labels = pd.Series(np.nan, index=df_docs.index, dtype=\"float\")\n",
    "    labels[papy] = 1.0\n",
    "    labels[gen & ~papy] = 0.0\n",
    "    return labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "acec562f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-11T15:13:30.248195Z",
     "start_time": "2025-11-11T15:13:30.185566Z"
    }
   },
   "outputs": [],
   "source": [
    "# --- Vectorization, LDA, and exports ---\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "\n",
    "def build_bigram_matrix(docs, min_df=2, max_df=0.90, use_tfidf=False):\n",
    "    \"\"\"Build bigram matrix using either raw counts or TF-IDF weighting.\n",
    "    \n",
    "    Args:\n",
    "        docs: List of document strings\n",
    "        min_df: Minimum document frequency threshold\n",
    "        max_df: Maximum document frequency threshold\n",
    "        use_tfidf: If True, use TF-IDF weighting instead of raw counts\n",
    "    \"\"\"\n",
    "    if use_tfidf:\n",
    "        from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "        vect = TfidfVectorizer(analyzer=bigram_analyzer, lowercase=False, min_df=min_df, max_df=max_df)\n",
    "    else:\n",
    "        vect = CountVectorizer(analyzer=bigram_analyzer, lowercase=False, min_df=min_df, max_df=max_df)\n",
    "    X = vect.fit_transform(docs)\n",
    "    return vect, X\n",
    "\n",
    "def top_terms_from_components(components, feature_names, topn=15):\n",
    "    topic_terms = []\n",
    "    for comp in components:\n",
    "        idx = np.argsort(comp)[::-1][:topn]\n",
    "        terms = [(feature_names[i], float(comp[i])) for i in idx]\n",
    "        topic_terms.append(terms)\n",
    "    return topic_terms\n",
    "\n",
    "def umass_coherence(components, X, topn=10, eps=1.0):\n",
    "    Xb = (X > 0).astype(int)\n",
    "    df = np.asarray(Xb.sum(axis=0)).ravel()\n",
    "    Xb_csr = Xb.tocsr()\n",
    "    coherences = []\n",
    "    for comp in components:\n",
    "        ids = np.argsort(comp)[::-1][:topn]\n",
    "        score = 0.0\n",
    "        pairs = 0\n",
    "        for i in range(len(ids)):\n",
    "            for j in range(i+1, len(ids)):\n",
    "                ti, tj = ids[i], ids[j]\n",
    "                docs_i = set(Xb_csr[:, ti].nonzero()[0])\n",
    "                docs_j = set(Xb_csr[:, tj].nonzero()[0])\n",
    "                dij = len(docs_i & docs_j)\n",
    "                score += np.log((dij + eps) / (df[ti] + eps))\n",
    "                pairs += 1\n",
    "        coherences.append(float(score / max(pairs, 1)))\n",
    "    return coherences\n",
    "\n",
    "def summarize_topic_sizes(theta):\n",
    "    hard = theta.argmax(axis=1)\n",
    "    K = theta.shape[1]\n",
    "    hard_count = np.bincount(hard, minlength=K)\n",
    "    mean_weight = theta.mean(axis=0)\n",
    "    return {\"hard_count\": hard_count, \"mean_weight\": mean_weight}\n",
    "\n",
    "def per_corpus_distribution(theta, labels):\n",
    "    hard = theta.argmax(axis=1)\n",
    "    K = theta.shape[1]\n",
    "    rows = []\n",
    "    for k in range(K):\n",
    "        mask_k = (hard == k)\n",
    "        papy = int(np.sum(mask_k & (labels == 1)))\n",
    "        gen  = int(np.sum(mask_k & (labels == 0)))\n",
    "        rows.append({\"topic\": k, \"docs_papyrology\": papy, \"docs_general\": gen})\n",
    "    return pd.DataFrame(rows)\n",
    "\n",
    "def run_pipeline(\n",
    "    input_dir: str,\n",
    "    output_dir: str = \"./outputs_rq2\",\n",
    "    aggregate_window: str = \"\",\n",
    "    ks = (20,25,30),\n",
    "    max_df: float = 0.90,\n",
    "    min_df: int = 2,\n",
    "    topn: int = 15,\n",
    "    papyrology_pattern: str = r\"papyrolog\",\n",
    "    general_pattern: str = r\"general\",\n",
    "    min_doc_len: int = 10,\n",
    "    use_tfidf: bool = False\n",
    "):\n",
    "    outdir = os.path.expanduser(output_dir)\n",
    "    os.makedirs(outdir, exist_ok=True)\n",
    "    print(\"[1/5] Load & cleanâ€¦\")\n",
    "    df = load_discord_dir(input_dir)\n",
    "    if df.empty:\n",
    "        print(\"No messages found.\")\n",
    "        return None\n",
    "    df = clean_messages(df, min_doc_len=min_doc_len)\n",
    "    if df.empty:\n",
    "        print(\"All messages filtered out.\")\n",
    "        return None\n",
    "\n",
    "    if aggregate_window:\n",
    "        print(f\"[2/5] Aggregating by {aggregate_window} per channelâ€¦\")\n",
    "        df_docs = aggregate_messages(df, aggregate_window)\n",
    "        docs = df_docs[\"text\"].tolist()\n",
    "        timestamps = df_docs[\"bucket_start\"].tolist()\n",
    "    else:\n",
    "        print(\"[2/5] No aggregation (per-message documents).\")\n",
    "        df_docs = df.copy()\n",
    "        docs = df_docs[\"text\"].tolist()\n",
    "        timestamps = df_docs[\"timestamp\"].tolist()\n",
    "\n",
    "    print(f\"Documents for LDA: {len(docs):,}\")\n",
    "\n",
    "    labels = label_corpora(df_docs, papyrology_pattern, general_pattern).to_numpy()\n",
    "\n",
    "    print(f\"[3/5] Building bigram matrixâ€¦ (matching your tokenizer/stopwords)\")\n",
    "    if use_tfidf:\n",
    "        print(\"  Using TF-IDF weighting (frequent terms will be downweighted)\")\n",
    "    else:\n",
    "        print(\"  Using raw counts\")\n",
    "    vect, X = build_bigram_matrix(docs, min_df=min_df, max_df=max_df, use_tfidf=use_tfidf)\n",
    "    feature_names = vect.get_feature_names_out()\n",
    "    print(f\"Vocabulary size (bigrams): {len(feature_names):,}\")\n",
    "    \n",
    "    # For coherence calculation, we need binary (presence/absence) matrix\n",
    "    # Convert to binary if using TF-IDF\n",
    "    X_binary = (X > 0).astype(int) if use_tfidf else X\n",
    "\n",
    "    outputs = []\n",
    "    for k in ks:\n",
    "        print(f\"[4/5] Fitting LDA with k={k}â€¦\")\n",
    "        lda = LatentDirichletAllocation(\n",
    "            n_components=k,\n",
    "            learning_method=\"batch\",\n",
    "            random_state=42,\n",
    "            doc_topic_prior=None,\n",
    "            topic_word_prior=None,\n",
    "            max_iter=50,\n",
    "            evaluate_every=0,\n",
    "        )\n",
    "        lda.fit(X)\n",
    "        theta = lda.transform(X)\n",
    "        comps = lda.components_\n",
    "\n",
    "        topic_terms = top_terms_from_components(comps, feature_names, topn=topn)\n",
    "        topic_sizes = summarize_topic_sizes(theta)\n",
    "        hard_count = topic_sizes[\"hard_count\"]\n",
    "        mean_weight = topic_sizes[\"mean_weight\"]\n",
    "\n",
    "        print(\"  Computing UMass coherenceâ€¦\")\n",
    "        # Use binary matrix for coherence (presence/absence)\n",
    "        coh = umass_coherence(comps, X_binary, topn=min(topn, 10))\n",
    "\n",
    "        comp_df = per_corpus_distribution(theta, labels)\n",
    "\n",
    "        rows = []\n",
    "        for t_id, terms in enumerate(topic_terms):\n",
    "            top_terms = \", \".join([w for (w, _) in terms])\n",
    "            top_probs = \", \".join([f\"{w}:{wt:.3f}\" for (w, wt) in terms])\n",
    "            pap = int(comp_df.loc[comp_df[\"topic\"] == t_id, \"docs_papyrology\"].iloc[0]) if not comp_df.empty else 0\n",
    "            gen = int(comp_df.loc[comp_df[\"topic\"] == t_id, \"docs_general\"].iloc[0]) if not comp_df.empty else 0\n",
    "            rows.append({\n",
    "                \"k\": k,\n",
    "                \"topic\": t_id,\n",
    "                \"docs_hard_count\": int(hard_count[t_id]),\n",
    "                \"docs_mean_weight\": float(mean_weight[t_id]),\n",
    "                \"coherence_umass\": float(coh[t_id]),\n",
    "                \"top_terms\": top_terms,\n",
    "                \"top_terms_with_weights\": top_probs,\n",
    "                \"docs_papyrology\": pap,\n",
    "                \"docs_general\": gen\n",
    "            })\n",
    "        topics_df = pd.DataFrame(rows).sort_values(\"docs_hard_count\", ascending=False)\n",
    "        out_csv = os.path.join(outdir, f\"topics_k{k}.csv\")\n",
    "        topics_df.to_csv(out_csv, index=False)\n",
    "        print(\"  Saved:\", out_csv)\n",
    "        outputs.append(out_csv)\n",
    "\n",
    "    print(\"\\n[5/5] Done âœ“\")\n",
    "    print(f\"- Docs: {len(docs):,}\")\n",
    "    print(f\"- Vocab(bigrams): {len(feature_names):,}\")\n",
    "    print(f\"- Ks tried: {list(ks)}\")\n",
    "    print(f\"- Outputs in: {os.path.abspath(outdir)}\")\n",
    "    print(f\"- Aggregation: {'ON ('+aggregate_window+')' if aggregate_window else 'OFF'}\")\n",
    "    print(f\"- Weighting: {'TF-IDF' if use_tfidf else 'Raw counts'}\")\n",
    "    print(f\"- Lemmatization: {'ON (spaCy)' if _SPACY_AVAILABLE else 'OFF (fallback tokens)'}\")\n",
    "    print(f\"- lower={getattr(cfg, 'lowercase', True)}, keep_only_alpha={getattr(cfg, 'keep_only_alpha', True)}, min_token_len={getattr(cfg, 'min_token_len', 2)}, remove_numbers={getattr(cfg, 'remove_numbers', True)}\")\n",
    "    return outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "22526d1e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-11T15:14:00.915031Z",
     "start_time": "2025-11-11T15:13:30.254580Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1/5] Load & cleanâ€¦\n",
      "[2/5] Aggregating by 30min per channelâ€¦\n",
      "Documents for LDA: 1,976\n",
      "[3/5] Building bigram matrixâ€¦ (matching your tokenizer/stopwords)\n",
      "  Using raw counts\n",
      "Vocabulary size (bigrams): 3,013\n",
      "[4/5] Fitting LDA with k=20â€¦\n",
      "  Computing UMass coherenceâ€¦\n",
      "  Saved: ./outputs_rq2/topics_k20.csv\n",
      "[4/5] Fitting LDA with k=25â€¦\n",
      "  Computing UMass coherenceâ€¦\n",
      "  Saved: ./outputs_rq2/topics_k25.csv\n",
      "[4/5] Fitting LDA with k=30â€¦\n",
      "  Computing UMass coherenceâ€¦\n",
      "  Saved: ./outputs_rq2/topics_k30.csv\n",
      "\n",
      "[5/5] Done âœ“\n",
      "- Docs: 1,976\n",
      "- Vocab(bigrams): 3,013\n",
      "- Ks tried: [20, 25, 30]\n",
      "- Outputs in: /Users/nikhil/PycharmProjects/vesuvius_discord_study/term_level/outputs_rq2\n",
      "- Aggregation: ON (30min)\n",
      "- Weighting: Raw counts\n",
      "- Lemmatization: ON (spaCy)\n",
      "- lower=True, keep_only_alpha=True, min_token_len=2, remove_numbers=True\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['./outputs_rq2/topics_k20.csv',\n",
       " './outputs_rq2/topics_k25.csv',\n",
       " './outputs_rq2/topics_k30.csv']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# --- Example: run the pipeline (edit paths as needed) ---\n",
    "outputs = run_pipeline(\n",
    "    input_dir=\"./filtered_JSON\",\n",
    "    output_dir=\"./outputs_rq2\",\n",
    "    aggregate_window=\"30min\",\n",
    "    ks=(20,25,30),\n",
    "    max_df=0.90,\n",
    "    min_df=2,\n",
    "    topn=15,\n",
    "    papyrology_pattern=r\"papyrolog\",\n",
    "    general_pattern=r\"general\",\n",
    "    min_doc_len=10,\n",
    ")\n",
    "outputs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7868c058",
   "metadata": {},
   "source": [
    "# ðŸ” Issue Analysis: Why \"ink detection\" appears in multiple topics\n",
    "\n",
    "**Problem:** Very frequent terms (like \"ink detection\") can appear across many topics in LDA because:\n",
    "1. They appear in many documents (high document frequency)\n",
    "2. LDA probabilistically assigns common terms to multiple topics to explain the data\n",
    "3. The current `max_df=0.90` only filters terms appearing in >90% of documents\n",
    "\n",
    "**Solutions:**\n",
    "1. **Lower `max_df`** to filter out more common terms (e.g., 0.50-0.60)\n",
    "2. **Use TF-IDF** instead of raw counts to downweight frequent terms\n",
    "3. **Add to stopwords** if it's domain-specific noise\n",
    "\n",
    "Let's first check the actual document frequency of \"ink detection\":\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "43def416",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ“Š 'ink detection' Statistics:\n",
      "  - Document Frequency: 126 / 1,976 documents (6.4%)\n",
      "  - Term Frequency: 150 total occurrences\n",
      "  - Current max_df threshold: 0.90 (1778 docs)\n",
      "\n",
      "âœ… 'ink detection' appears in 6.4% of documents\n",
      "   With max_df=0.90, it's kept because 6.4% < 90%\n",
      "\n",
      "ðŸ’¡ Solution: Lower max_df to 0.06 or below to filter it out\n"
     ]
    }
   ],
   "source": [
    "# Diagnostic: Check document frequency of \"ink detection\"\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "# Re-run the pipeline setup to get the vectorizer\n",
    "input_dir = \"./filtered_JSON\"\n",
    "df = load_discord_dir(input_dir)\n",
    "df = clean_messages(df, min_doc_len=10)\n",
    "df_docs = aggregate_messages(df, \"30min\")\n",
    "docs = df_docs[\"text\"].tolist()\n",
    "\n",
    "# Build vectorizer with current settings\n",
    "vect, X = build_bigram_matrix(docs, min_df=2, max_df=0.90)\n",
    "feature_names = vect.get_feature_names_out()\n",
    "\n",
    "# Find \"ink detection\" in vocabulary\n",
    "ink_detection_idx = None\n",
    "for i, name in enumerate(feature_names):\n",
    "    if name == \"ink detection\":\n",
    "        ink_detection_idx = i\n",
    "        break\n",
    "\n",
    "if ink_detection_idx is not None:\n",
    "    # Calculate document frequency (how many documents contain this term)\n",
    "    X_binary = (X[:, ink_detection_idx] > 0).astype(int)\n",
    "    doc_freq = X_binary.sum()\n",
    "    doc_freq_pct = (doc_freq / len(docs)) * 100\n",
    "    \n",
    "    # Calculate term frequency (total occurrences)\n",
    "    term_freq = X[:, ink_detection_idx].sum()\n",
    "    \n",
    "    print(f\"ðŸ“Š 'ink detection' Statistics:\")\n",
    "    print(f\"  - Document Frequency: {doc_freq:,} / {len(docs):,} documents ({doc_freq_pct:.1f}%)\")\n",
    "    print(f\"  - Term Frequency: {term_freq:,} total occurrences\")\n",
    "    print(f\"  - Current max_df threshold: 0.90 ({int(0.90 * len(docs))} docs)\")\n",
    "    print(f\"\\nâœ… 'ink detection' appears in {doc_freq_pct:.1f}% of documents\")\n",
    "    print(f\"   With max_df=0.90, it's kept because {doc_freq_pct:.1f}% < 90%\")\n",
    "    print(f\"\\nðŸ’¡ Solution: Lower max_df to {(doc_freq_pct/100):.2f} or below to filter it out\")\n",
    "else:\n",
    "    print(\"âš ï¸ 'ink detection' not found in vocabulary (may have been filtered)\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5089719",
   "metadata": {},
   "source": [
    "## Solution 1: Lower `max_df` to filter common terms\n",
    "\n",
    "Run the pipeline with a lower `max_df` (e.g., 0.50 or 0.60) to filter out terms that appear in more than 50-60% of documents. This will remove \"ink detection\" and other overly common terms.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8d050814",
   "metadata": {},
   "outputs": [],
   "source": [
    "# #Solution 1: Re-run with lower max_df (uncomment to run)\n",
    "# outputs_filtered = run_pipeline(\n",
    "#     input_dir=\"./filtered_JSON\",\n",
    "#     output_dir=\"./outputs_rq2_filtered\",  # Different output directory\n",
    "#     aggregate_window=\"30min\",\n",
    "#     ks=(20,25,30),\n",
    "#     max_df=0.50,  # Lower threshold - filters terms in >50% of documents\n",
    "#     min_df=2,\n",
    "#     topn=15,\n",
    "#     papyrology_pattern=r\"papyrolog\",\n",
    "#     general_pattern=r\"general\",\n",
    "#     min_doc_len=10,\n",
    "# )\n",
    "# outputs_filtered\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "671acb79",
   "metadata": {},
   "source": [
    "## Solution 2: Use TF-IDF weighting instead of raw counts\n",
    "\n",
    "TF-IDF (Term Frequency-Inverse Document Frequency) automatically downweights frequent terms, which naturally reduces their dominance across topics. This requires modifying the LDA pipeline to use `TfidfVectorizer` instead of `CountVectorizer`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e1201757",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Solution 2: TF-IDF version - Now integrated into run_pipeline()!\n",
    "# The run_pipeline() function now accepts use_tfidf=True parameter\n",
    "# TF-IDF automatically downweights frequent terms like \"ink detection\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "951ad018",
   "metadata": {},
   "source": [
    "## Solution 3: Add \"ink detection\" to stopwords\n",
    "\n",
    "If \"ink detection\" is domain-specific noise that doesn't help distinguish topics, you can add it directly to the stopword list in the configuration.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8a8d2a6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Solution 3: Add \"ink detection\" to stopwords\n",
    "# In the Cfg class above, modify extra_stopwords:\n",
    "# extra_stopwords: Set[str] = {\n",
    "#     \"server\", \"joined\", \"scroll\", \"papyrus\", \"image\", \"brett\", \"olsen\",\n",
    "#     \"entry\", \"start\", \"thread\", \"moshe\", \"levy\", \"casey\", \"handmer\", \"mae\",\n",
    "#     \"sawatzky\", \"like\", \"value\", \"seldon\", \"ben\",\n",
    "#     \"ink\", \"detection\"  # Add individual words, or filter bigrams separately\n",
    "# }\n",
    "\n",
    "# However, since you're using bigrams, you'd need to filter \"ink detection\" \n",
    "# after bigram generation. A better approach is Solution 1 (lower max_df).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "861d815f",
   "metadata": {},
   "source": [
    "## âœ… Run LDA with TF-IDF\n",
    "\n",
    "Use `use_tfidf=True` in `run_pipeline()` to apply TF-IDF weighting. This will automatically downweight frequent terms like \"ink detection\", reducing their dominance across topics.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "227dc560",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1/5] Load & cleanâ€¦\n",
      "[2/5] Aggregating by 30min per channelâ€¦\n",
      "Documents for LDA: 1,976\n",
      "[3/5] Building bigram matrixâ€¦ (matching your tokenizer/stopwords)\n",
      "  Using TF-IDF weighting (frequent terms will be downweighted)\n",
      "Vocabulary size (bigrams): 3,013\n",
      "[4/5] Fitting LDA with k=20â€¦\n",
      "  Computing UMass coherenceâ€¦\n",
      "  Saved: ./outputs_rq2_tfidf/topics_k20.csv\n",
      "[4/5] Fitting LDA with k=25â€¦\n",
      "  Computing UMass coherenceâ€¦\n",
      "  Saved: ./outputs_rq2_tfidf/topics_k25.csv\n",
      "[4/5] Fitting LDA with k=30â€¦\n",
      "  Computing UMass coherenceâ€¦\n",
      "  Saved: ./outputs_rq2_tfidf/topics_k30.csv\n",
      "\n",
      "[5/5] Done âœ“\n",
      "- Docs: 1,976\n",
      "- Vocab(bigrams): 3,013\n",
      "- Ks tried: [20, 25, 30]\n",
      "- Outputs in: /Users/nikhil/PycharmProjects/vesuvius_discord_study/term_level/outputs_rq2_tfidf\n",
      "- Aggregation: ON (30min)\n",
      "- Weighting: TF-IDF\n",
      "- Lemmatization: ON (spaCy)\n",
      "- lower=True, keep_only_alpha=True, min_token_len=2, remove_numbers=True\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['./outputs_rq2_tfidf/topics_k20.csv',\n",
       " './outputs_rq2_tfidf/topics_k25.csv',\n",
       " './outputs_rq2_tfidf/topics_k30.csv']"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Run LDA with TF-IDF weighting\n",
    "outputs_tfidf = run_pipeline(\n",
    "    input_dir=\"./filtered_JSON\",\n",
    "    output_dir=\"./outputs_rq2_tfidf\",  # Different output directory\n",
    "    aggregate_window=\"30min\",\n",
    "    ks=(20,25,30),\n",
    "    max_df=0.90,  # Can keep max_df=0.90 since TF-IDF will downweight frequent terms\n",
    "    min_df=2,\n",
    "    topn=15,\n",
    "    papyrology_pattern=r\"papyrolog\",\n",
    "    general_pattern=r\"general\",\n",
    "    min_doc_len=10,\n",
    "    use_tfidf=True  # ðŸŽ¯ Enable TF-IDF weighting\n",
    ")\n",
    "outputs_tfidf\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
