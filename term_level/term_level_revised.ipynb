{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "dc8bd4f8",
   "metadata": {},
   "source": [
    "# Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8e6d0e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords as nltk_stopwords\n",
    "import re\n",
    "import json\n",
    "import spacy\n",
    "\n",
    "with open('stopwords-iso.json', 'r', encoding='utf-8') as file:\n",
    "    stopwords_iso = json.load(file)\n",
    "\n",
    "stopwords = set(nltk_stopwords.words('english'))\n",
    "custom_stopwords = set(['server', 'joined', 'scroll', 'papyri', 'image', 'brett', 'olsen', 'entry', 'start', 'thread', 'moshe', 'levy', 'casey', 'handmer', 'mae', 'sawatzky'])\n",
    "stopwords.update(custom_stopwords)\n",
    "stopwords.update(stopwords_iso['en'])\n",
    "\n",
    "def preprocess_text(text):\n",
    "\n",
    "    # Convert text to lowercase\n",
    "    text = text.lower()\n",
    "    \n",
    "    # Load SpaCy model\n",
    "    nlp = spacy.load(\"en_core_web_sm\")\n",
    "    doc = nlp(text)\n",
    "\n",
    "    # Lemmatize and lowercase each token\n",
    "    text = ' '.join([token.lemma_.lower() for token in doc])\n",
    "    \n",
    "    # Remove function definitions\n",
    "    text = re.sub(r'\\b[a-zA-Z_][a-zA-Z0-9_]*\\s*\\([^)]*\\)\\s*', '', text)\n",
    "    \n",
    "    # Remove URLs\n",
    "    text = re.sub(r'http[s]?://\\S+', '', text)\n",
    "\n",
    "    # Remove file names with common extensions\n",
    "    text = re.sub(r'\\b\\w+\\.(zip|tif|pdf|jpg|png|docx|xlsx|rar|txt|csv|json)\\b', '', text)\n",
    "\n",
    "    # Remove numbers\n",
    "    text = re.sub(r'\\d+', '', text)\n",
    "\n",
    "    # Remove punctuation\n",
    "    text = re.sub(r'[^\\w\\s]', '', text)\n",
    "\n",
    "    # Remove special characters and emojis\n",
    "    text = re.sub(r'[\\U0001F600-\\U0001F64F'\n",
    "              r'\\U0001F300-\\U0001F5FF'  \n",
    "              r'\\U0001F680-\\U0001F6FF'  \n",
    "              r'\\U0001F700-\\U0001F77F'  \n",
    "              r'\\U0001F780-\\U0001F7FF'  \n",
    "              r'\\U0001F800-\\U0001F8FF'  \n",
    "              r'\\U0001F900-\\U0001F9FF'  \n",
    "              r'\\U0001FA00-\\U0001FA6F'  \n",
    "              r'\\U0001FA70-\\U0001FAFF'  \n",
    "              r'\\u2600-\\u26FF'          \n",
    "              r'\\u2700-\\u27BF'       \n",
    "              ']+', '', text)\n",
    "\n",
    "    # Remove stopwords\n",
    "    tokens = text.split()\n",
    "    tokens = [token for token in tokens if token not in stopwords]\n",
    "    \n",
    "    return ' '.join(tokens)\n",
    "\n",
    "def preprocess_json(input_file, output_file):\n",
    "    with open(input_file, 'r', encoding='utf-8') as file:\n",
    "        data = json.load(file)\n",
    "\n",
    "    for message in data.get('messages', []):\n",
    "        if message.get('content'):\n",
    "            message['content'] = preprocess_text(message['content'])\n",
    "\n",
    "    with open(output_file, 'w', encoding='utf-8') as file:\n",
    "        json.dump(data, file, indent=4)\n",
    "\n",
    "    print(f'Preprocessed data has been saved to {output_file}')\n",
    "\n",
    " \n",
    "def preprocess_all_files(input_folder, output_folder):\n",
    "    for filename in os.listdir(input_folder):\n",
    "        input_file = os.path.join(input_folder, filename)\n",
    "        output_file = os.path.join(output_folder, filename)\n",
    "        preprocess_json(input_file, output_file)\n",
    "\n",
    "\n",
    "input_folder = 'filteredJSON'\n",
    "output_folder = 'preprocessedJSON'\n",
    "\n",
    "preprocess_all_files(input_folder, output_folder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d36048f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Additional stopword cleanup for when new ones need to be added so we don't have to reprocess everything\n",
    "additional_stopwords = set(['mae', 'sawatzky'])  # Add any new terms here\n",
    "input_folder = 'preprocessedJSON'\n",
    "output_folder = 'preprocessedJSON'\n",
    "\n",
    "def remove_additional_stopwords(input_folder, output_folder):\n",
    "    for filename in os.listdir(input_folder):\n",
    "        input_file = os.path.join(input_folder, filename)\n",
    "        output_file = os.path.join(output_folder, filename)\n",
    "\n",
    "        with open(input_file, 'r', encoding='utf-8') as file:\n",
    "            data = json.load(file)\n",
    "\n",
    "        for message in data.get('messages', []):\n",
    "            if message.get('content'):\n",
    "                tokens = message['content'].split()\n",
    "                tokens = [token for token in tokens if token not in additional_stopwords]\n",
    "                message['content'] = ' '.join(tokens)\n",
    "\n",
    "        with open(output_file, 'w', encoding='utf-8') as file:\n",
    "            json.dump(data, file, indent=4)\n",
    "\n",
    "        print(f'Additional stopwords removed from {filename}')\n",
    "\n",
    "remove_additional_stopwords(input_folder, output_folder)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75f82751",
   "metadata": {},
   "source": [
    "# Organizing Messages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "3b60d573",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "input_folder = 'preprocessedJSON'\n",
    "def get_all_documents(folder):\n",
    "    all_docs = []\n",
    "    files =  {'Vesuvius Challenge - Text Channels - papyrology [1108134343295127592]_filtered.json', \n",
    "              'Vesuvius Challenge - Text Channels - general [1079907750265499772]_filtered.json'} #for looking at specific files, can customize\n",
    "\n",
    "    for fname in os.listdir(folder):\n",
    "        if fname not in files:\n",
    "            continue\n",
    "\n",
    "        with open(os.path.join(folder, fname), 'r', encoding='utf-8') as f:\n",
    "            data = json.load(f)\n",
    "            channel = data.get(\"channel\", \"Unknown\")\n",
    "            channel_name = channel.get(\"name\") if isinstance(channel, dict) else channel\n",
    "\n",
    "            for msg in data.get(\"messages\", []):\n",
    "                content = msg.get(\"content\", \"\")\n",
    "                if content.strip():  # Only include non-empty messages\n",
    "                    all_docs.append({\n",
    "                        \"channel\": channel_name,\n",
    "                        \"user\": msg.get(\"author\", {}).get(\"name\", \"Unknown\"),\n",
    "                        \"timestamp\": msg.get(\"timestamp\", \"Unknown\"),\n",
    "                        \"content\": content\n",
    "                    })\n",
    "\n",
    "    return all_docs\n",
    "\n",
    "\n",
    "df = pd.DataFrame(get_all_documents(input_folder))\n",
    "\n",
    "df['timestamp'] = pd.to_datetime(df['timestamp'], errors='coerce')\n",
    "# Remove timezone only if present\n",
    "if df['timestamp'].dt.tz is not None:\n",
    "    df['timestamp'] = df['timestamp'].dt.tz_localize(None)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afa6981c",
   "metadata": {},
   "source": [
    "# Term Frequency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3607be0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from collections import Counter\n",
    "from nltk.util import ngrams\n",
    "import spacy\n",
    "import re\n",
    "import unicodedata\n",
    "\n",
    "# Load SpaCy model\n",
    "spacy.cli.download(\"en_core_web_sm\")\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "\n",
    "def is_greek_script(token_text):\n",
    "    return any('GREEK' in unicodedata.name(c, '') for c in token_text)\n",
    "\n",
    "def tokenize(text):\n",
    "    doc = nlp(text.lower())\n",
    "    tokens = []\n",
    "\n",
    "    for token in doc:\n",
    "        if token.is_alpha:\n",
    "            if is_greek_script(token.text):\n",
    "                tokens.append('[GREEK_LANGUAGE]')\n",
    "            else:\n",
    "                tokens.append(token.text)\n",
    "    return tokens\n",
    "\n",
    "\n",
    "df['tokens'] = df['content'].apply(tokenize)\n",
    "\n",
    "# Compute term frequency and document frequency\n",
    "def get_ngram_stats(token_lists, n=1):\n",
    "    term_freq = Counter()\n",
    "    doc_freq = Counter()\n",
    "\n",
    "    for tokens in token_lists:\n",
    "        if n == 1:\n",
    "            terms = tokens\n",
    "        else:\n",
    "            terms = list(ngrams(tokens, n))\n",
    "\n",
    "        term_freq.update(terms)\n",
    "        doc_freq.update(set(terms))  # Count each term once per document\n",
    "\n",
    "    return term_freq, doc_freq\n",
    "\n",
    "# Convert stats to DataFrame\n",
    "def ngram_df_with_stats(term_freq, doc_freq, label='ngram', top_k=50):\n",
    "    df = pd.DataFrame({\n",
    "        label: list(term_freq.keys()),\n",
    "        'term_frequency': list(term_freq.values()),\n",
    "        'document_frequency': [doc_freq[term] for term in term_freq]\n",
    "    })\n",
    "    df.sort_values(by='term_frequency', ascending=False, inplace=True)\n",
    "    return df.head(top_k)\n",
    "\n",
    "\n",
    "# Split by channel\n",
    "general_msgs = df[df['channel'] == 'general']['tokens'].dropna().tolist()\n",
    "papyrology_msgs = df[df['channel'] == 'papyrology']['tokens'].dropna().tolist()\n",
    "combined_msgs = general_msgs + papyrology_msgs\n",
    "\n",
    "# Get stats\n",
    "tf_uni_comb, df_uni_comb = get_ngram_stats(combined_msgs, n=1)\n",
    "tf_bi_comb, df_bi_comb = get_ngram_stats(combined_msgs, n=2)\n",
    "\n",
    "tf_uni_gen, df_uni_gen = get_ngram_stats(general_msgs, n=1)\n",
    "tf_uni_pap, df_uni_pap = get_ngram_stats(papyrology_msgs, n=1)\n",
    "\n",
    "tf_bi_gen, df_bi_gen = get_ngram_stats(general_msgs, n=2)\n",
    "tf_bi_pap, df_bi_pap = get_ngram_stats(papyrology_msgs, n=2)\n",
    "\n",
    "# Write to Excel\n",
    "with pd.ExcelWriter('channel_analysis3.xlsx') as writer:\n",
    "    df.to_excel(writer, sheet_name='Messages', index=False)\n",
    "\n",
    "    # Combined bigrams\n",
    "    ngram_df_with_stats(tf_bi_comb, df_bi_comb, 'bigram').to_excel(writer, sheet_name='Bigrams Combined', index=False)\n",
    "\n",
    "    # Separate bigrams\n",
    "    pd.concat([\n",
    "        ngram_df_with_stats(tf_bi_gen, df_bi_gen, 'bigram').assign(channel='general'),\n",
    "        ngram_df_with_stats(tf_bi_pap, df_bi_pap, 'bigram').assign(channel='papyrology')\n",
    "    ]).to_excel(writer, sheet_name='Bigrams Separate', index=False)\n",
    "\n",
    "    # Combined unigrams\n",
    "    ngram_df_with_stats(tf_uni_comb, df_uni_comb, 'unigram').to_excel(writer, sheet_name='Unigrams Combined', index=False)\n",
    "\n",
    "    # Separate unigrams\n",
    "    pd.concat([\n",
    "        ngram_df_with_stats(tf_uni_gen, df_uni_gen, 'unigram').assign(channel='general'),\n",
    "        ngram_df_with_stats(tf_uni_pap, df_uni_pap, 'unigram').assign(channel='papyrology')\n",
    "    ]).to_excel(writer, sheet_name='Unigrams Separate', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "187d8c37",
   "metadata": {},
   "source": [
    "# Statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43111454",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import matplotlib.dates as mdates\n",
    "\n",
    "# Read in the dataframe\n",
    "df = pd.read_excel('channel_analysis.xlsx', sheet_name='Messages')\n",
    "\n",
    "# Normalize channel names\n",
    "df['channel'] = df['channel'].str.lower()\n",
    "\n",
    "# Define subsets\n",
    "channels = {\n",
    "    'General': df[df['channel'] == 'general'],\n",
    "    'Papyrology': df[df['channel'] == 'papyrology'],\n",
    "    'Both': df\n",
    "}\n",
    "\n",
    "# Loop through each subset\n",
    "for name, subset in channels.items():\n",
    "    print(f\"\\n--- {name} Channel ---\")\n",
    "    print(f\"Total messages: {len(subset)}\")\n",
    "\n",
    "    # Convert timestamp and sort\n",
    "    subset = subset.copy()\n",
    "    subset['timestamp'] = pd.to_datetime(subset['timestamp'], errors='coerce')\n",
    "    subset = subset.dropna(subset=['timestamp'])\n",
    "    subset = subset.set_index('timestamp').sort_index()\n",
    "\n",
    "    # Messages per day\n",
    "    messages_per_day = subset.resample('D').size()\n",
    "\n",
    "    # Reindex to include all days in range (even with zero messages)\n",
    "    start_date = subset.index.min().normalize()\n",
    "    end_date = subset.index.max().normalize()\n",
    "    all_days = pd.date_range(start=start_date, end=end_date, freq='D')\n",
    "    messages_per_day = messages_per_day.reindex(all_days, fill_value=0)\n",
    "\n",
    "    # Plot with narrow bars and month labels\n",
    "    plt.figure(figsize=(18, 6))\n",
    "    plt.bar(messages_per_day.index, messages_per_day.values, width=0.8, color='skyblue')\n",
    "    plt.title(f'{name} Messages Over Time')\n",
    "    plt.xlabel('Month')\n",
    "    plt.ylabel('Number of Messages')\n",
    "\n",
    "    # Format x-axis to show month names\n",
    "    ax = plt.gca()\n",
    "    ax.xaxis.set_major_locator(mdates.MonthLocator())\n",
    "    ax.xaxis.set_major_formatter(mdates.DateFormatter('%b'))\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    # Activity by weekday and hour\n",
    "    subset['weekday'] = subset.index.weekday\n",
    "    subset['hour'] = subset.index.hour\n",
    "    activity = subset.groupby(['weekday', 'hour']).size().unstack(fill_value=0)\n",
    "\n",
    "    plt.figure(figsize=(14, 6))\n",
    "    sns.heatmap(activity, cmap='YlGnBu')\n",
    "    plt.title(f'{name} Activity by Weekday and Hour')\n",
    "    plt.xlabel('Hour of Day')\n",
    "    plt.ylabel('Weekday (0=Monday)')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    # Top users\n",
    "    top_users = subset['user'].value_counts().head(10)\n",
    "    plt.figure(figsize=(14, 6))\n",
    "    sns.barplot(x=top_users.values, y=top_users.index, palette='viridis')\n",
    "    plt.title(f'{name} Top 10 Users by Number of Messages')\n",
    "    plt.xlabel('Number of Messages')\n",
    "    plt.ylabel('User')\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
