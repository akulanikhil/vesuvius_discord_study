{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Streamlined Term-Level Analysis (spaCy + NLTK)\n",
    "- spaCy tokenization & **lemmatization**\n",
    "- NLTK + custom stopwords\n",
    "- **Per-document** bigrams\n",
    "- URL / emoji / simple system-message filtering\n",
    "- Unigram & bigram **TF/DF** + **MI**"
   ],
   "id": "85df2ee816080257"
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-08T18:44:00.598420Z",
     "start_time": "2025-10-08T18:44:00.401500Z"
    }
   },
   "source": [
    "# !pip install spacy nltk pandas tqdm\n",
    "# !python -m spacy download en_core_web_sm\n",
    "\n",
    "import re, math, os\n",
    "from collections import Counter\n",
    "from typing import Iterable, List, Tuple, Set, Dict\n",
    "import pandas as pd\n",
    "\n",
    "import nltk\n",
    "try:\n",
    "    nltk.data.find('corpora/stopwords')\n",
    "except LookupError:\n",
    "    nltk.download('stopwords')\n",
    "\n",
    "import spacy\n",
    "\n",
    "def load_spacy():\n",
    "    try:\n",
    "        return spacy.load(\"en_core_web_sm\")\n",
    "    except Exception:\n",
    "        nlp = spacy.blank(\"en\")\n",
    "        try:\n",
    "            nlp.add_pipe(\"lemmatizer\", config={\"mode\":\"lookup\"})\n",
    "        except Exception:\n",
    "            pass\n",
    "        print(\"Using blank 'en' pipeline (install en_core_web_sm for better lemmatization).\")\n",
    "        return nlp\n",
    "\n",
    "nlp = load_spacy()"
   ],
   "id": "33a01aca9c28351c",
   "outputs": [],
   "execution_count": 27
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-08T18:44:00.609491Z",
     "start_time": "2025-10-08T18:44:00.606512Z"
    }
   },
   "source": [
    "class Config:\n",
    "    lowercase = True\n",
    "    keep_only_alpha = True\n",
    "    min_token_len = 2\n",
    "    use_lemma = True\n",
    "    remove_numbers = True\n",
    "    \n",
    "    # Stopwords\n",
    "    use_nltk_stopwords = True\n",
    "    extra_stopwords = {\n",
    "        \"server\", \"joined\", \"scroll\", \"papyrus\", \"image\", \"brett\", \"olsen\", \"entry\", \"start\", \"thread\", \"moshe\", \"levy\", \"casey\", \"handmer\", \"mae\", \"sawatzky\", \"like\", \"value\", \"seldon\", \"ben\"\n",
    "    }\n",
    "    \n",
    "    # Filters\n",
    "    system_message_patterns = [\n",
    "        re.compile(r\"^\\s*Joined the server\\.?\\s*$\", re.I),\n",
    "        re.compile(r\"^\\s*Started a thread\\.?\\s*$\", re.I),\n",
    "    ]\n",
    "    url_inline_re = re.compile(r\"(https?://\\S+|www\\.\\S+)\", re.I)\n",
    "    url_only_re   = re.compile(r\"^\\s*(https?://\\S+\\s*)+$\", re.I)\n",
    "    emoji_re = re.compile(\n",
    "        \"[\"\n",
    "        \"\\U0001F300-\\U0001F5FF\"\n",
    "        \"\\U0001F600-\\U0001F64F\"\n",
    "        \"\\U0001F680-\\U0001F6FF\"\n",
    "        \"\\U0001F700-\\U0001F77F\"\n",
    "        \"\\U0001F780-\\U0001F7FF\"\n",
    "        \"\\U0001F800-\\U0001F8FF\"\n",
    "        \"\\U0001F900-\\U0001F9FF\"\n",
    "        \"\\U0001FA00-\\U0001FA6F\"\n",
    "        \"\\U0001FA70-\\U0001FAFF\"\n",
    "        \"\\U00002702-\\U000027B0\"\n",
    "        \"\\U000024C2-\\U0001F251\"\n",
    "        \"]+\")\n",
    "    greek_re = re.compile(r\"[\\u0370-\\u03FF\\u1F00-\\u1FFF]\")\n",
    "        \n",
    "cfg = Config()\n",
    "\n",
    "def contains_greek(text: str) -> bool:\n",
    "    return bool(cfg.greek_re.search(text or \"\"))\n"
   ],
   "id": "8bd853475687012d",
   "outputs": [],
   "execution_count": 28
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-08T18:44:00.618683Z",
     "start_time": "2025-10-08T18:44:00.615048Z"
    }
   },
   "source": [
    "from nltk.corpus import stopwords as nltk_stop\n",
    "\n",
    "def build_stopword_set() -> Set[str]:\n",
    "    sw = set()\n",
    "    if cfg.use_nltk_stopwords:\n",
    "        sw |= set(nltk_stop.words(\"english\"))\n",
    "    sw |= set(cfg.extra_stopwords)\n",
    "    return {w.lower() for w in sw}\n",
    "\n",
    "STOPWORDS = build_stopword_set()\n",
    "len(STOPWORDS)"
   ],
   "id": "e032f8a45db10823",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "218"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 29
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-08T18:44:00.632855Z",
     "start_time": "2025-10-08T18:44:00.629870Z"
    }
   },
   "source": [
    "def is_system_message(text: str) -> bool:\n",
    "    for pat in cfg.system_message_patterns:\n",
    "        if pat.search(text):\n",
    "            return True\n",
    "    return False\n",
    "\n",
    "def strip_urls(text: str) -> str:\n",
    "    if cfg.url_only_re.match(text or \"\"):\n",
    "        return \"\"\n",
    "    return cfg.url_inline_re.sub(\" \", text or \"\")\n",
    "\n",
    "def strip_emoji(text: str) -> str:\n",
    "    return cfg.emoji_re.sub(\" \", text or \"\")\n",
    "\n",
    "def clean_text(text: str) -> str:\n",
    "    if not text:\n",
    "        return \"\"\n",
    "    if is_system_message(text):\n",
    "        return \"\"\n",
    "    text = strip_urls(text)\n",
    "    text = strip_emoji(text)\n",
    "    return re.sub(r\"\\s+\", \" \", text).strip()"
   ],
   "id": "9378c7d0c0fb0ca",
   "outputs": [],
   "execution_count": 30
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-08T18:44:00.648295Z",
     "start_time": "2025-10-08T18:44:00.645159Z"
    }
   },
   "source": [
    "def doc_to_tokens(doc_text: str, nlp, stopwords: Set[str]) -> List[str]:\n",
    "    t = clean_text(doc_text)\n",
    "    if not t:\n",
    "        return []\n",
    "    sp = nlp(t)\n",
    "    toks = []\n",
    "    for token in sp:\n",
    "        if cfg.keep_only_alpha and not token.text.isalpha():\n",
    "            continue\n",
    "        if cfg.remove_numbers and any(ch.isdigit() for ch in token.text):\n",
    "            continue\n",
    "        raw = token.text.lower() if cfg.lowercase else token.text\n",
    "        norm = token.lemma_.lower() if (cfg.use_lemma and token.lemma_) else raw\n",
    "        if contains_greek(norm):\n",
    "            norm = \"GREEK_LETTERS\"  # keep uppercase on purpose\n",
    "        if len(norm) < cfg.min_token_len:\n",
    "            continue\n",
    "        if norm in stopwords:\n",
    "            continue\n",
    "        toks.append(norm)\n",
    "    return toks\n",
    "\n",
    "def docs_to_token_lists(texts: Iterable[str]) -> List[List[str]]:\n",
    "    return [doc_to_tokens(t, nlp, STOPWORDS) for t in texts]"
   ],
   "id": "fcff44b565f802db",
   "outputs": [],
   "execution_count": 31
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-08T18:44:00.655245Z",
     "start_time": "2025-10-08T18:44:00.653083Z"
    }
   },
   "source": [
    "def bigrams_from_tokens_per_doc(tokens_per_doc: List[List[str]]) -> List[List[Tuple[str,str]]]:\n",
    "    bigram_docs = []\n",
    "    for toks in tokens_per_doc:\n",
    "        bigram_docs.append([(toks[i], toks[i+1]) for i in range(len(toks)-1)] if len(toks) > 1 else [])\n",
    "    return bigram_docs"
   ],
   "id": "fe884cae05bc754c",
   "outputs": [],
   "execution_count": 32
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-08T18:44:00.662072Z",
     "start_time": "2025-10-08T18:44:00.659780Z"
    }
   },
   "source": [
    "def unigram_counts(tokens_per_doc: List[List[str]]):\n",
    "    tf, df = Counter(), Counter()\n",
    "    for toks in tokens_per_doc:\n",
    "        tf.update(toks)\n",
    "        df.update(set(toks))\n",
    "    return tf, df\n",
    "\n",
    "def bigram_counts(bigrams_per_doc: List[List[Tuple[str,str]]]):\n",
    "    tf, df = Counter(), Counter()\n",
    "    for bigs in bigrams_per_doc:\n",
    "        tf.update(bigs)\n",
    "        df.update(set(bigs))\n",
    "    return tf, df"
   ],
   "id": "44a44988f9db139a",
   "outputs": [],
   "execution_count": 33
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-08T18:44:00.668155Z",
     "start_time": "2025-10-08T18:44:00.665887Z"
    }
   },
   "source": [
    "def mutual_information_bigrams(unigram_tf: Counter, bigram_tf: Counter, smoothing: int = 1):\n",
    "    T = sum(bigram_tf.values()) + smoothing\n",
    "    mi = {}\n",
    "    for (w1,w2), c12 in bigram_tf.items():\n",
    "        c1 = unigram_tf.get(w1, 0)\n",
    "        c2 = unigram_tf.get(w2, 0)\n",
    "        num = (c12 + smoothing) * T\n",
    "        den = (c1 + smoothing) * (c2 + smoothing)\n",
    "        mi[(w1,w2)] = math.log2(num / den)\n",
    "    return mi"
   ],
   "id": "e8ddc241e31ce161",
   "outputs": [],
   "execution_count": 34
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-08T18:44:00.676022Z",
     "start_time": "2025-10-08T18:44:00.671970Z"
    }
   },
   "source": [
    "def analyze_corpus(docs, top_k=5000, output_dir=\"/Users/nikhil/PycharmProjects/vesuvius_discord_study/term_level/outputs\", pretokenized=False):\n",
    "    \"\"\"\n",
    "    docs: either Iterable[str] (if pretokenized=False) or List[List[str]] (if pretokenized=True)\n",
    "    \"\"\"\n",
    "    if pretokenized:\n",
    "        tokens_per_doc = docs\n",
    "    else:\n",
    "        tokens_per_doc = docs_to_token_lists(docs)\n",
    "\n",
    "    bigrams_per_doc = bigrams_from_tokens_per_doc(tokens_per_doc)\n",
    "    uni_tf, uni_df = unigram_counts(tokens_per_doc)\n",
    "    bi_tf,  bi_df  = bigram_counts(bigrams_per_doc)\n",
    "\n",
    "    df_uni = (pd.DataFrame([(t, uni_tf[t], uni_df[t]) for t in uni_tf], columns=[\"term\",\"tf\",\"df\"])\n",
    "                .sort_values([\"tf\",\"df\",\"term\"], ascending=[False,False,True]).head(top_k))\n",
    "    df_bi = (pd.DataFrame([(\" \".join(p), bi_tf[p], bi_df[p]) for p in bi_tf], columns=[\"bigram\",\"tf\",\"df\"])\n",
    "                .sort_values([\"tf\",\"df\",\"bigram\"], ascending=[False,False,True]).head(top_k))\n",
    "\n",
    "    mi = mutual_information_bigrams(uni_tf, bi_tf, smoothing=1)\n",
    "    df_mi = (pd.DataFrame([(f\"{w1} {w2}\", bi_tf[(w1,w2)], m) for (w1,w2), m in mi.items()], columns=[\"bigram\",\"tf\",\"mi\"])\n",
    "                .sort_values([\"mi\",\"tf\",\"bigram\"], ascending=[False,False,True]).head(top_k))\n",
    "\n",
    "    if output_dir:\n",
    "        os.makedirs(output_dir, exist_ok=True)\n",
    "        df_uni.to_csv(os.path.join(output_dir, \"unigrams.csv\"), index=False)\n",
    "        df_bi.to_csv(os.path.join(output_dir, \"bigrams.csv\"), index=False)\n",
    "        df_mi.to_csv(os.path.join(output_dir, \"bigrams_mi.csv\"), index=False)\n",
    "\n",
    "    return {\"unigrams\": df_uni, \"bigrams\": df_bi, \"bigrams_mi\": df_mi}\n"
   ],
   "id": "da113841cac54049",
   "outputs": [],
   "execution_count": 35
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## Loading JSON and running term analysis",
   "id": "b47c045eaede64a9"
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-08T18:44:33.123320Z",
     "start_time": "2025-10-08T18:44:00.682293Z"
    }
   },
   "source": [
    "import json\n",
    "import os\n",
    "\n",
    "def load_json_records_with_tokens(folder: str) -> List[Dict]:\n",
    "    \"\"\"\n",
    "    Returns records with full-pipeline normalization:\n",
    "      - URLs/emoji/system-message stripping\n",
    "      - spaCy tokenization + lemmatization\n",
    "      - NLTK+custom stopwords\n",
    "    Output schema:\n",
    "      { author, timestamp, channel, raw, tokens, context }\n",
    "    where `context` is a comma-separated string of tokens.\n",
    "    \"\"\"\n",
    "    records = []\n",
    "    files = [f for f in os.listdir(folder) if f.lower().endswith(\".json\")]\n",
    "    for fname in files:\n",
    "        fpath = os.path.join(folder, fname)\n",
    "        try:\n",
    "            with open(fpath, \"r\", encoding=\"utf-8\") as f:\n",
    "                data = json.load(f)\n",
    "        except Exception as e:\n",
    "            print(f\"⚠️ Skipped {fname}: {e}\")\n",
    "            continue\n",
    "\n",
    "        channel = (data.get(\"channel\", {}) or {}).get(\"name\") or data.get(\"name\") or os.path.splitext(fname)[0]\n",
    "        for m in data.get(\"messages\", []):\n",
    "            raw = (m.get(\"content\") or \"\").strip()\n",
    "            if not raw:\n",
    "                continue\n",
    "\n",
    "            # Full normalization through your doc_to_tokens pipeline\n",
    "            tokens = doc_to_tokens(raw, nlp, STOPWORDS)\n",
    "            if not tokens:\n",
    "                continue\n",
    "\n",
    "            author = (m.get(\"author\") or {}).get(\"name\") or (m.get(\"author\") or {}).get(\"id\") or \"Unknown\"\n",
    "            ts     = m.get(\"timestamp\") or \"\"\n",
    "            records.append({\n",
    "                \"author\": author,\n",
    "                \"timestamp\": ts,\n",
    "                \"channel\": channel,\n",
    "                \"raw\": raw,\n",
    "                \"tokens\": tokens,\n",
    "                # 🔽 change: comma-separated tokens rather than space-joined\n",
    "                \"context\": \", \".join(tokens),\n",
    "            })\n",
    "    print(f\"Loaded {len(records)} normalized messages from {len(files)} files.\")\n",
    "    return records\n",
    "\n",
    "records = load_json_records_with_tokens(\"/Users/nikhil/PycharmProjects/vesuvius_discord_study/term_level/filtered_JSON\")\n",
    "docs_tokens = [r[\"tokens\"] for r in records]  # pretokenized\n"
   ],
   "id": "92e2e4374ba6705d",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 7649 normalized messages from 2 files.\n"
     ]
    }
   ],
   "execution_count": 36
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-08T18:44:36.343249Z",
     "start_time": "2025-10-08T18:44:33.146711Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "results = analyze_corpus(docs_tokens, top_k=100000, output_dir=\"/Users/nikhil/PycharmProjects/vesuvius_discord_study/term_level/outputs\", pretokenized=True)\n",
    "\n",
    "\n",
    "# Build the cleaned-messages DataFrame\n",
    "df_msgs = pd.DataFrame(\n",
    "    [{k: r[k] for k in [\"author\",\"timestamp\",\"channel\",\"raw\",\"context\"]} for r in records],\n",
    "    columns=[\"author\",\"timestamp\",\"channel\",\"raw\",\"context\"]\n",
    ")\n",
    "output_dir=\"/Users/nikhil/PycharmProjects/vesuvius_discord_study/term_level/outputs\"\n",
    "xlsx_path = os.path.join(output_dir, \"term_level_analysis.xlsx\")\n",
    "\n",
    "# Choose an engine\n",
    "engine = None\n",
    "try:\n",
    "    import xlsxwriter  # noqa\n",
    "    engine = \"xlsxwriter\"\n",
    "except Exception:\n",
    "    engine = \"openpyxl\"\n",
    "\n",
    "with pd.ExcelWriter(xlsx_path, engine=engine) as writer:\n",
    "    results[\"unigrams\"].to_excel(writer, sheet_name=\"unigrams\", index=False)\n",
    "    results[\"bigrams\"].to_excel(writer,  sheet_name=\"bigrams\",  index=False)\n",
    "    results[\"bigrams_mi\"].to_excel(writer, sheet_name=\"bigrams_mi\", index=False)\n",
    "    df_msgs.to_excel(writer, sheet_name=\"messages_cleaned\", index=False)\n",
    "\n",
    "print(\"Wrote Excel to:\", xlsx_path)\n",
    "\n",
    "# Quick sanity check: how many rows remained identical after FULL normalization?\n",
    "same = (df_msgs[\"raw\"] == df_msgs[\"context\"]).sum()\n",
    "print(f\"Rows where raw==context after full normalization: {same} / {len(df_msgs)}\")\n",
    "df_msgs.head(5)\n"
   ],
   "id": "f79f640bafc2634b",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wrote Excel to: /Users/nikhil/PycharmProjects/vesuvius_discord_study/term_level/outputs/term_level_analysis.xlsx\n",
      "Rows where raw==context after full normalization: 114 / 7649\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "        author                      timestamp  channel  \\\n",
       "0        mjq13  2023-03-15T03:44:06.141+00:00  general   \n",
       "1  natfriedman  2023-03-15T04:26:56.585+00:00  general   \n",
       "2  natfriedman  2023-03-15T04:27:10.993+00:00  general   \n",
       "3  natfriedman  2023-03-15T04:29:56.175+00:00  general   \n",
       "4  natfriedman  2023-03-15T15:48:38.026+00:00  general   \n",
       "\n",
       "                                                 raw  \\\n",
       "0  I see this sentence  \"To make it easier to try...   \n",
       "1                    Good find, we will remove that!   \n",
       "2  We are now providing the data files untarred s...   \n",
       "3  Each .tif file in the full scroll sans is 8 mi...   \n",
       "4                                             Hello!   \n",
       "\n",
       "                                             context  \n",
       "0  see, sentence, make, easy, try, datum, also, r...  \n",
       "1                                 good, find, remove  \n",
       "2  provide, datum, file, untarred, fetch, whateve...  \n",
       "3  file, full, san, micrometer, tall, want, grab,...  \n",
       "4                                              hello  "
      ],
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>author</th>\n",
       "      <th>timestamp</th>\n",
       "      <th>channel</th>\n",
       "      <th>raw</th>\n",
       "      <th>context</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>mjq13</td>\n",
       "      <td>2023-03-15T03:44:06.141+00:00</td>\n",
       "      <td>general</td>\n",
       "      <td>I see this sentence  \"To make it easier to try...</td>\n",
       "      <td>see, sentence, make, easy, try, datum, also, r...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>natfriedman</td>\n",
       "      <td>2023-03-15T04:26:56.585+00:00</td>\n",
       "      <td>general</td>\n",
       "      <td>Good find, we will remove that!</td>\n",
       "      <td>good, find, remove</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>natfriedman</td>\n",
       "      <td>2023-03-15T04:27:10.993+00:00</td>\n",
       "      <td>general</td>\n",
       "      <td>We are now providing the data files untarred s...</td>\n",
       "      <td>provide, datum, file, untarred, fetch, whateve...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>natfriedman</td>\n",
       "      <td>2023-03-15T04:29:56.175+00:00</td>\n",
       "      <td>general</td>\n",
       "      <td>Each .tif file in the full scroll sans is 8 mi...</td>\n",
       "      <td>file, full, san, micrometer, tall, want, grab,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>natfriedman</td>\n",
       "      <td>2023-03-15T15:48:38.026+00:00</td>\n",
       "      <td>general</td>\n",
       "      <td>Hello!</td>\n",
       "      <td>hello</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 37
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "file_extension": ".py",
   "mimetype": "text/x-python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
